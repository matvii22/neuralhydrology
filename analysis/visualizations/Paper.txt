







CP193 - Revised Full Draft
Minerva University
CP194 - Capstone Directed Study II
Prof. Morgan
March 1, 2025










CP193 - Revised Full Draft
Table of Contents
Introduction	3
What is an LSTM?	9
LSTM Application in Hydrology	10
Neural Hydrology Library	12
CAMELS Dataset	13
Feature-Engineering Focus	14
Future Directions	16
References	18
HCs and LOs Appendix	20




















Introduction
The United Kingdom faces a growing crisis of increasing flood frequency and severity, positioning the nation at the forefront of climate change's hydrological impacts. Multiple studies across the UK indicate that flood events have become more frequent and severe over recent decades, creating an urgent need for improved prediction and mitigation strategies. In the Bristol and Bath area, flood hazards are estimated to be 19–49% higher in near-term and future periods compared to historical baselines, while analyses in the Severn Uplands and Thames Basin project flow increases ranging from approximately 5% to 50% depending on local geology and catchment characteristics (Archer et al., 2024; Kay et al., 2008).
The UK's vulnerability to flooding comes from its distinctive geographical and meteorological characteristics. As an island nation with extensive coastlines, numerous river systems, and high floodplain population density, the country faces compounded risks from multiple flood sources. Statistical analyses from northern and western regions consistently report significant upward trends in high‐flow indicators and peak river flows, creating a notable north-south divide in flood risk evolution (Hannaford & Marsh, 2008; Hannaford et al., 2021). Geological differences further complicate this regional variation. Kay et al. (2008) observe that non-chalk areas in the Thames Basin face potential flow increases of 30–50%, while chalk areas might experience more modest gains of 5–10%.
Climate change plays a central role in these observed and projected flood patterns. Model simulations and climate change scenarios support projections of increased flood frequency, as changes in precipitation patterns and extreme rainfall events directly impact flood hazard estimates and flow increases. Biggs (2009) notes seasonal shifts expressed in increases in winter and autumn precipitation alongside decreases in summer precipitation, suggesting evolving seasonal flood risk profiles. These changes align with the expected effects of climate change on UK weather patterns, particularly the intensification of the hydrological cycle and the increased frequency of extreme precipitation events.
The UK also experiences increasingly more compound flood events. Halliday et al. (2020) investigated the relationship between extreme winds and inland flooding, finding more compound events than would be expected by chance. This finding suggests that climate change may increase the frequency of individual flood events and the likelihood of multiple hazards coinciding, potentially overwhelming existing flood defense systems.
Archer et al. (2024) predict increased flood hazards for both near-term (2021-2040) and long-term (2061-2080) periods, while Biggs (2009) projects flow increases of up to 30% by the 2080s. This trend is particularly concerning given the UK's concentrated population and infrastructure in flood-prone areas, with urban settings facing distinct challenges compared to rural catchments (Griffin et al., 2022).
In this context of established links between climate change and increasing flood hazards, Mohanty et al. (2024) research introduces a novel perspective. Their paper "Temperature Variability and Natural Disasters" challenges the conventional focus on absolute temperature increases by suggesting that temperature variability, rather than mean temperature levels, could be the primary predictor of natural disaster occurrence. Using a long-differences regression approach across 176 countries and the contiguous United States from 1960 to 2018, they found that temperature variability (measured by standard deviation) demonstrates superior predictive power compared to mean temperature changes.
This finding raises important questions for UK flood risk assessment, particularly given the country's already observed increases in weather volatility. However, Mohanty et al.'s research has limitations relevant to the UK context. It does not adequately address disaster severity—a critical consideration given the UK's concentrated exposure in flood-prone areas—and its 30-year averaging approach may mask shorter-term trends that could be particularly relevant in the UK's dynamic climate system.
In this paper, we will analyze how temperature variability and other meteorological indicators can be used to predict the occurrence and severity of natural disasters in the United Kingdom, focusing on flooding events. By leveraging the rich landscape of UK-specific hydrological, meteorological, and geographical data, we aim to develop a more nuanced understanding of disaster risk factors within this regional context. Our methodology will incorporate domain adaptation techniques to enhance existing prediction frameworks with locally relevant features, potentially improving upon generic models trained on global datasets. We will systematically evaluate whether such regionalized approaches result in noticeable performance improvements over baseline methods, particularly for extreme event prediction. Additionally, we will explore the development of a precise method and guidelines that allow for continuous model refinement as new environmental observations become available. By focusing on a single country with well-documented climate and disaster data, this research will investigate whether temperature variability and other climate and hydrological metrics pooled from a specific location offer improved predictive power for the UK's specific flood risk profile.
	
	Traditional physics-based hydrological models
	Physics-based hydrological models represent water movement through mathematical equations derived from physical laws. These models solve equations for the conservation of mass, momentum, and energy across landscape elements. The fundamental principle involves partitioning rainfall into different hydrological processes. Water inputs from precipitation follow pathways, including surface runoff, infiltration, evapotranspiration, and groundwater flow. Specific differential equations with defined boundary conditions represent each process. The models simulate how water moves through the landscape based on topography, soil properties, and vegetation characteristics.
Physical representation creates advantages for modeling diverse conditions. The models rely on established hydraulic and hydrodynamic principles rather than purely statistical relationships. Parameters in physics-based models have physical meaning, such as soil porosity or hydraulic conductivity. The underlying equations remain valid across different climate scenarios and extreme events.
Physics-based flood modeling in the UK began developing significantly in the 1970s and 1980s. Early models faced severe computational limitations and simplified many processes. Models like the Institute of Hydrology's HYRROM represented basic rainfall-runoff relationships with physical parameters. The UK Flood Studies Report (1975) established foundational approaches for flood estimation based on physical catchment characteristics. Advances in computational power through the 1990s enabled more sophisticated representations of physical processes.
The UK Environment Agency operates the Grid-to-Grid (G2G) model as a significant operational physics-based system. G2G operates on a high-resolution 1 km² grid across the UK, providing detailed local flood predictions that support specific community warning systems (Centre for Ecology & Hydrology, 2020). The model translates rainfall into river flows by incorporating detailed physical data to predict when rivers will overtop their banks. Topography, soil properties, geology, and land cover data feed directly into model parameters to reflect real-world conditions. G2G routes runoff through grid cells to simulate river flows in a physically consistent manner, helping emergency services anticipate which areas will flood and when. The model supports forecasts extending lead times from 2 to 5 days compared to previous approaches, giving communities additional time to prepare (UK Centre for Ecology & Hydrology, 2021).
The Global Flood Awareness System (GloFAS) provides complementary large-scale forecasting that helps coordinate international response efforts. GloFAS integrates the HTESSEL land surface model with the LISFLOOD hydrological model to provide consistent forecasts across national boundaries. The system simulates surface runoff, subsurface flow, groundwater processes, and river routing to predict flooding in transboundary river basins where coordinated response is essential. GloFAS operates at a coarser resolution (~11 km grid) than national systems but covers larger areas to support continental-scale planning (Alfieri et al., 2013). The model produces ensemble forecasts up to 30 days ahead and seasonal outlooks up to 16 weeks, supporting long-term resource allocation and preparedness activities. European collaboration through the Copernicus Emergency Management Service supports the system's development and operation to ensure consistent cross-border flood management approaches.
Advanced physically-based models like SHETRAN offer even more detailed process representation for specific high-risk watersheds. SHETRAN has been applied to 698 UK catchments with full physical representation of hydrological processes, providing tailored predictions for areas with complex flood dynamics (Lewis et al., 2018). The model uses first-principle equations for integrated surface and subsurface water dynamics to simulate how water will move through multiple interconnected pathways. Physical parameters directly represent measurable catchment properties rather than calibrated coefficients, making the model more robust for simulating unprecedented events.
Climate change impacts have driven enhanced capabilities for simulating extreme events beyond historical ranges, helping communities prepare for unprecedented flooding scenarios (Kay et al., 2021). Modern physics-based models can represent human interventions like reservoirs and flood defense structures, allowing engineers to test different operating strategies during extreme events. Higher resolution topographic data from LiDAR (Light Detection and Ranging) surveys has enabled a more accurate physical representation of floodplains, identifying vulnerable properties down to individual building levels.
Computational advances allow ensemble modeling approaches with physics-based systems to improve how we quantify forecast confidence. Multiple simulations with varied parameters help quantify forecast uncertainty, allowing emergency managers to simultaneously prepare for best and worst-case scenarios. Parallel computing enables higher-resolution simulations that capture fine-scale topographic features such as small levees and drainage channels significantly influencing local flood patterns. Data assimilation techniques have improved, making it possible to have real-time updates of model states using observations from river gauges and weather radar to continually refine predictions during an evolving flood event. The UK Centre for Ecology & Hydrology continues developing faster algorithms for efficient simulations across multiple scenarios, helping decision-makers evaluate different response options quickly during emergencies.
Despite the strengths of physics-based models and how they represent the actual environments, the growing complexity of flood prediction challenges has resulted in the growing exploration efforts of artificial intelligence approaches to the problem. Machine learning methods offer ways to address computational limitations while capturing complex patterns in hydrological data. Long Short-Term Memory (LSTM) networks are particularly well-suited for flood prediction tasks because their recurrent architecture explicitly models temporal dependencies across multiple timescales. The ability of LSTMs to "remember" relevant conditions from previous timesteps while "forgetting" irrelevant information reflects the physical reality of catchment memory effects, where antecedent moisture conditions significantly influence flood response. 
	What is an LSTM?
Long Short-Term Memory (LSTM) networks represent a specialized class of recurrent neural networks developed to address the limitations of traditional neural networks in processing sequential data. Hochreiter and Schmidhuber (1997) introduced LSTMs to solve the vanishing gradient problem that prevented earlier recurrent networks from learning long-term dependencies. The fundamental architecture of LSTMs includes a memory cell and three specialized gates—input, forget, and output—that regulate information flow. The memory cell acts as a reservoir that can maintain information over extended sequences. The forget gate determines which information should be discarded from the cell state. The input gate controls which new information enters the cell state. The output gate regulates which parts of the cell state influence the network's output.
This architectural design enables LSTMs to selectively retain relevant historical information while discarding irrelevant details—a capability especially valuable for hydrological modeling. The network learns which antecedent conditions are meaningful predictors for future streamflow through exposure to historical examples rather than through explicitly programmed physical relationships. LSTM networks process input sequences incrementally, updating their internal state at each time step based on new inputs and previous memory content. Training occurs through backpropagation through time, where prediction errors propagate backward through the sequence to adjust network weights. This learning process allows LSTMs to identify complex, non-linear relationships between meteorological inputs and hydrological responses.
LSTM Application in Hydrology
A notable strength of LSTM approaches is their ability to maintain accuracy when predicting extreme events, even when they are underrepresented in training data. Frame et al. (2021) showed that LSTM models remained relatively accurate for flood events outside their training distribution, contrasting with traditional models that often struggle with unprecedented conditions. Li et al. (2020) successfully applied LSTMs to high-temporal-resolution forecasting at 15-minute intervals, demonstrating flexibility across different prediction timescales.
	
Figure 1. Diagram of a Long Short-Term Memory (LSTM) network architecture constrained to conserve physical quantities (Nearing et al., 2020). LSTMs process sequential data through gates (sigmoid functions σ̂) that regulate information flow between timesteps. The memory state (Xt) selectively incorporates new inputs (Ut) while preserving relevant historical information. Each gate serves a specific purpose: controlling input integration, memory retention, and output generation based on the current state.
The LSTM structure shown in Figure 1 works especially well for modeling watershed and river systems. Water in a watershed moves through various pathways - some fast (like surface runoff) and some slow (like groundwater) - before reaching streams. Similarly, LSTMs manage information through different pathways. The input gate decides how much new rainfall data should influence the current prediction. The forget gate determines which historical conditions (like last week's soil moisture) stay relevant. The output gate controls how this combined information produces streamflow predictions.
We can think of watersheds as having "memory" - today's river flow depends on yesterday's conditions and sometimes conditions from weeks ago. LSTMs mirror this property through their memory cells. For example, a heavy rainfall event immediately affects streamflow and influences groundwater levels for weeks afterward. Traditional models struggle to capture these varying time scales, but LSTMs excel at learning which past events matter and for how long. The gates in the diagram represent mathematical functions that learn these relationships from data rather than requiring manual programming of physical equations.
When applied to flood prediction in the UK, LSTM networks can process long rainfall sequences, temperature, and soil moisture data to forecast river levels days in advance. Their ability to "remember" seasonal patterns and watershed conditions gives them an advantage over simpler statistical approaches, especially for extreme events where complex interactions between multiple factors determine flood severity.
Neural Hydrology Library
In the scope of this research, we will be using the NeuralHydrology Python library to train LSTM models and analyze how fine-tuning and altering datasets with additional metrics influences the model outcomes. This library provides a specialized framework for training neural networks in hydrological applications. Developed by the AI for Earth Science group at Johannes Kepler University in Austria and researchers at Google, it has become a standard tool in hydrological machine learning research. NeuralHydrology incorporates state-of-the-art LSTM architectures specifically designed for streamflow prediction.
NeuralHydrology prioritizes modularity through configuration files rather than code modifications. Users define model parameters, data sources, and training settings in YAML configuration files without touching the underlying code. The library handles all implementation details, including data loading, preprocessing, model architecture setup, training loops, and evaluation metrics. The framework builds on PyTorch, allowing researchers to implement custom features when needed.
Kratzert et al.'s (2024) recent paper "Never train a Long Short-Term Memory (LSTM) network on a single basin" challenges established practices in hydrological modeling. Their research demonstrates that LSTM models fundamentally differ from traditional physics-based models regarding data requirements. While conventional models perform best when calibrated for individual watersheds, LSTMs achieve superior results when trained on data from many basins simultaneously. Despite this evidence, their survey revealed that 81% of LSTM papers published between 2021-2023 still used single-basin training approaches.
This persistence of single-basin training stems from researchers applying traditional hydrological intuitions to machine learning. Physics-based models cannot effectively handle diverse data from multiple watersheds due to their parameter calibration requirements. Many researchers incorrectly assume the same limitation applies to LSTMs. The multi-basin advantage becomes clearer when examining model performance on extreme events. Models trained on diverse data encountered a wider range of hydrological conditions, making them more capable of predicting rare flood events that might be poorly represented in any single basin's historical record.
The paper presents compelling evidence through comparative performance metrics. LSTM models trained on 531 CAMELS basins achieved median Nash-Sutcliffe Efficiency (NSE) scores approximately 0.05-0.15 higher than those trained on individual basins. This performance gap widens when evaluating extreme event prediction, with multi-basin models capturing flow patterns that single-basin models systematically underestimate.

CAMELS Dataset
In this research, we will primarily use the CAMELS (Catchment Attributes and Meteorology for Large-sample Studies) dataset. It is a comprehensive dataset designed to support large-sample hydrological studies. It provides a standardized set of hydrometeorological time series and catchment attributes, including topography, climate, streamflow, land cover, soil, and geology data. The publicly available dataset aims to facilitate hydrological modeling and analysis by offering a consistent and detailed set of data for various regions.
CAMELS-GB is the first large-sample catchment hydrology dataset specifically tailored for Great Britain. It integrates river flows, catchment attributes, and catchment boundaries from the UK National River Flow Archive with new meteorological time series and catchment attributes. The dataset covers 671 catchments (basins)  across Great Britain and provides daily time series for various hydro-meteorological variables from 1970 to 2015.
CAMELS-GB includes a wide range of data. It contains daily data for rainfall, potential evapotranspiration, temperature, radiation, humidity, and river flow. It also provides comprehensive catchment attributes such as elevation, drainage path slope, land cover types, soil types, and hydrogeological productivity classes. Additionally, it includes human management attributes, such as data on abstractions, returns, and reservoir capacity. The dataset also provides the first set of discharge uncertainty estimates for Great Britain, offered at multiple flow quantiles.
The Neural Hydrology library's tight integration with the CAMELS-GB dataset simplifies training an initial large-scale model. This integration benefits our research as it allows us to efficiently experiment with feature engineering and combined metrics tailored to UK-specific hydrological conditions. This streamlined workflow supports continuous model refinement and improves the accuracy of flood predictions.
Feature-Engineering Focus
Research by Kratzert et al. (2019) pointed to the absence of a structured approach to transferring features between datasets from different regions, such as CAMELS-US and CAMELS-GB, which could improve model performance through cross-regional learning.
Similarly, Wilbrand et al. (2023) explored the use of global meteorological forcing data with multi-timescale LSTM models, finding that integrating diverse datasets could enhance accuracy. Yet, this approach fell short of addressing basin-specific adaptations that consider unique hydrological behaviors and extreme event histories. The findings from these studies suggest that while feature integration can improve model accuracy, the lack of systematic, location-specific feature engineering limits the full potential of LSTM networks in hydrological forecasting.
Further complicating this issue is the argument presented by Kratzert et al. (2024) that training LSTM models on single-basin datasets is inherently flawed due to the narrow range of hydrological behaviors captured in such data. Their research showed that multi-basin-trained models outperformed single-basin models and were more effective at predicting extreme events. It can be attributed to the greater diversity in hydrological responses present in multi-basin datasets, which expands the training envelope of the model, enabling it to generalize better to unseen conditions. This insight suggests that a hybrid approach, combining multi-basin training with detailed basin-specific feature engineering, could bridge the current gap in model performance. By retaining the advantages of diverse training data while incorporating customized features that reflect local conditions, it may be possible to develop robust and sensitive models for regional hydrological dynamics.
This research aims to systematically explore the impact of feature engineering and combined metrics on the predictive capabilities of LSTM models for UK flood forecasting. By integrating static catchment attributes, such as soil type and land use, and dynamic meteorological variables, including rainfall and temperature, this study aims to develop a more nuanced understanding of how these features influence flood predictions. Furthermore, introducing custom location-aware metrics, such as indices for soil moisture deficit and antecedent precipitation, is expected to improve the model’s ability to predict both regular and extreme events more accurately. The hypothesis driving this research is that a systematic approach to feature engineering, including cross-dataset feature transfer and custom location-aware metrics, can significantly enhance the predictive accuracy of LSTM models for individual basins.
Developing a systematic framework for feature engineering and combined metrics could provide a transferable methodology for improving flood predictions in other regions with similarly diverse hydroclimatic conditions. 


Future Directions
Looking ahead, several key directions for future research emerge. First, integrating additional environmental indicators could enhance the model's predictive power. Particularly promising are sea surface temperature anomalies from the UK Met Office and atmospheric pressure patterns from ECMWF. Second, developing compound event frameworks could better capture the complex interactions between environmental factors that often precede significant disasters.
Current findings suggest that implementing a 2-3 month lagged indicator window could improve predictive accuracy. This aligns with observed patterns in the relationship between soil moisture peaks and flood events, where most floods occur within a 10-month window following significant soil moisture anomalies.
Additionally, incorporating socioeconomic vulnerability metrics could provide valuable context for risk assessment. This might include population density maps, infrastructure vulnerability indices, and historical land use changes. Such contextual data could help refine risk predictions and better inform disaster preparedness efforts.
Methodologically, future work will focus on developing more sophisticated weighting schemes that adapt to seasonal variations and long-term climate trends. The current linear weighting approach, while effective, could be enhanced by incorporating non-linear relationships between environmental indicators and disaster risk.
Finally, the validation and refinement of thresholds used in the risk-scoring system present another vital avenue for research. The current thresholds (197.8 for soil moisture and 95.9 for rainfall) have shown discriminative power, but further calibration could improve their predictive accuracy.
This focused approach to floods and storms, combined with the weighted severity prediction model, provides a solid foundation for continued research. The precise identification of future research directions ensures that subsequent work will systematically address current limitations while building on the established strengths of the current methodology.
This research trajectory promises to enhance our understanding of disaster risk dynamics and has practical implications for disaster preparedness and response strategies. The ongoing refinement of these models could significantly improve our ability to anticipate and mitigate the impacts of floods and storms in the UK context.



















References
Arnell, N. W., Kay, A. L., Freeman, A., Rudd, A. C., & Lowe, J. A. (2020). Changing climate risk in the UK: a multi-sectoral analysis using policy-relevant indicators. Climate Risk Management, 31, 100265. https://doi.org/10.1016/j.crm.2020.100265
Bates, P. D., Savage, J., Wing, O., Quinn, N., Sampson, C., Neal, J., & Smith, A. (2023). A climate-conditioned catastrophe risk model for UK flooding. Natural Hazards and Earth System Sciences, 23(2), 891–908. https://doi.org/10.5194/nhess-23-891-2023
Hughes, K. (2020, February 18). Storms inflict £7.7bn worth of damage on a third of UK property. The Independent. https://www.independent.co.uk/money/spend-save/storms-ciara-dennis-ellen-damage-flooding-trees-roof-tiles-insurance-claims-a9341536.html
Jones, R. L., Guha-Sapir, D., & Tubeuf, S. (2022). Human and economic impacts of natural disasters: can we trust the global data? Scientific Data, 9(1). https://doi.org/10.1038/s41597-022-01667-x
Kendon, M., McCarthy, M., Jevrejeva, S., Matthews, A., Williams, J., Sparks, T. H., & Fraser. (2023). State of the UK Climate 2022. International Journal of Climatology, 43(S1), 1–83. https://doi.org/10.1002/joc.8167
Kochkov, D., Yuval, J., Langmore, I., Norgaard, P., Smith, J., Mooers, G., Klöwer, M., Lottes, J., Rasp, S., Düben, P., Hatfield, S., Battaglia, P., Sanchez-Gonzalez, A., Willson, M., Brenner, M. P., & Hoyer, S. (2024). Neural general circulation models for weather and climate. Nature. https://doi.org/10.1038/s41586-024-07744-y
Lam, R., Sanchez-Gonzalez, A., Willson, M., Wirnsberger, P., Fortunato, M., Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Hu, W., Merose, A., Hoyer, S., Holland, G. A., Oriol Vinyals, Stott, J., Pritzel, A., Mohamed, S., & Battaglia, P. W. (2023). Learning skillful medium-range global weather forecasting. Science, 382(6677). https://doi.org/10.1126/science.adi2336
Mohanty, A., Powdthavee, N., Tang, C. K., & Oswald, A. J. (2024). Temperature Variability and Natural Disasters. ArXiv.org. https://arxiv.org/abs/2409.14936
Nearing, G., Cohen, D., Dube, V., Gauch, M., Gilon, O., Harrigan, S., Avinatan Hassidim, Klotz, D., Kratzert, F., Metzger, A., Sella Nevo, Florian Pappenberger, Prudhomme, C., Shalev, G., Shlomo Shenzis, Tadele Yednkachw Tekalign, Weitzner, D., & Matias, Y. (2024). Global prediction of extreme floods in ungauged watersheds. Nature, 627(8004), 559–563. https://doi.org/10.1038/s41586-024-07145-1
Price, I., Sanchez-Gonzalez, A., Alet, F., Andersson, T. R., El-Kadi, A., Masters, D., Ewalds, T., Stott, J., Mohamed, S., Battaglia, P., Lam, R., & Willson, M. (2023). GenCast: Diffusion-based ensemble forecasting for medium-range weather. ArXiv.org. https://arxiv.org/abs/2312.15796
Price, I., Sanchez-Gonzalez, A., Alet, F., Andersson, T. R., El-Kadi, A., Masters, D., Ewalds, T., Stott, J., Mohamed, S., Battaglia, P., Lam, R., & Willson, M. (2024). Probabilistic weather forecasting with machine learning. Nature. https://doi.org/10.1038/s41586-024-08252-9





HCs and LOs Appendix
HCs and LOs plan - Capstone

HCs and LOs to be graded are marked with green fill color.
		
#emergentproperties
The research effectively captures emergent properties in the relationship between temperature patterns and disaster occurrence, primarily through its sophisticated feature engineering approach. While individual weather metrics provide fundamental insights, the paper demonstrates how combining multiple indicators creates complex interaction patterns that emerge at a macro level. The model's feature engineering reveals how compound event indicators and interaction terms between temperature and rainfall metrics produce emergent risk patterns that exceed simple linear combinations. We can spot this in the analysis of floods and storms, where soil saturation indices combined with temperature variability and rainfall patterns create higher-order predictive signals that we cannot derive from examining these factors in isolation. The research shows how these engineered features, including threshold-based extreme weather indicators and rolling window analyses, capture emergent behaviors that single-metric approaches would miss. The temporal integration of these combined metrics through the 2-3 month lagged indicator windows demonstrates how the interaction of multiple environmental factors creates emergent disaster risk patterns that require sophisticated modeling approaches to predict effectively.
#variables
The research demonstrates refined variable identification and classification throughout the methodology section. The independent variables include three key temperature metrics (maximum, minimum, mean, and variability) alongside supporting meteorological variables like rainfall and soil moisture measurements. I separated the target dependent variable into two parts: disaster occurrence (binary classification) and disaster severity (categorized into five levels from No Deaths to High). I address confounding variables in the Data Limitations section by acknowledging how reporting biases and socioeconomic factors might influence the relationship between temperature patterns and disaster outcomes. The research strengthens its analysis by incorporating temporal variables (pre/post-2010 analysis) and geographical parameters (UK-specific context) to control for potential confounding effects. I clearly classify quantitative variables (temperature measurements, casualty counts) and qualitative variables (disaster types, severity categories), guiding the appropriate choice of statistical methods throughout the analysis.
#correlation
The research extensively utilizes correlation analysis throughout the correlation and predictive modeling sections. The paper presents detailed correlation findings between temperature metrics and disaster-related deaths, showing maximum temperature correlation (Pearson: 0.316, p=0.037), temperature variability correlation (Pearson: 0.307, p=0.042), and minimum temperature correlation (Pearson: 0.250, p=0.102). The analysis becomes more sophisticated when examining temporal changes, revealing how correlations shifted between pre-2010 and post-2010 periods, with maximum temperature correlation weakening from r=0.404 to r=0.373 and temperature variability showing an even more dramatic decrease from r=0.441 to r=0.225. The research uses correlation for prediction through its random forest classifier model, which employs multiple correlation trees to predict disaster occurrence and severity while avoiding overfitting through ensemble averaging.
#constraints
The research demonstrates a vigorous application of constraints throughout the methodology and model development sections. I recognize several key limitations inherent to the data and type of research I'm conducting:
Data availability constraints from the EM-DAT database, particularly pre-2000 records, as well as the limitations presented by the extensive missing data, as identified in the paper by Jones et al. (2022).
Geographic constraints by focusing specifically on UK territory, primarily resulting in limited data and inability to extrapolate the odel to other countries.
Computational constraints leading to the choice of a random forest classifier and then weighted scoring model over more complex models, like a transformer.
Temporal constraints in the correlation analysis, splitting data into pre and post-2010 periods. These constraints guided crucial methodological decisions, such as focusing on specific disaster types (floods and storms) and choosing appropriate model architectures. Recognizing and incorporating these constraints strengthened the research by ensuring the solutions remained practical and implementable within the available resources.
#evidencebased
I refer to high-quality academic sources when forming arguments about the problem's relevance and use established data sources while also explaining their limitations, particularly when analyzing temperature variability's relationship with natural disasters. The correlation analysis section systematically presents statistical evidence through visual and numerical data, using scatter plots and correlation coefficients to support key findings. When discussing the emerging significance of extreme temperature events, the paper cites specific statistics - three heat waves between 2021 and 2023 contributing to 7,876 deaths (86.3% of all temperature-related deaths since 1980), as well as the floods, storms, and extreme temperature events making up 93.9% of all disasters in the country. I structure the evidence comprehensively, moving from broad disaster trends to specific temperature-mortality relationships, with each section building on facts before introducing new analyses. Moreover, I visualize the data to enhance my analysis and identify patterns in the data, with plots 5-7 providing clear evidence of the relationships between temperature metrics and disaster severity.
#rightproblem
The paper effectively characterizes the complex problem of predicting natural disasters through temperature variability in the introduction and data analysis sections. I define the initial state of the current AI advancements in natural disaster prediction and extreme weather events in the UK through a comprehensive analysis of historical disaster data and temperature patterns in the UK. I center the goal state around developing more accurate prediction models for disaster occurrence and severity, identifying the latter as a missing piece in the paper on temperature variability and its connection to the frequency of natural disasters. The paper identifies key obstacles, particularly in the Data Limitations section, highlighting challenges with missing data and reporting inconsistencies that affect prediction accuracy. I narrow the scale of my problem to the context of the UK while acknowledging the benefits and limitations that choice introduces, presented by the focused analysis of UK-specific meteorological data and disaster records. Moreover, after conducting extensive statistical analysis and attempting to create a model that predicts natural disasters, I decided to focus on floods and storms. These disaster types are most prevalent in the country and have similar features, making it easier to use shared parameters to predict both types of events.
#biasmitigation
The research actively addresses potential biases in natural disaster data analysis and modeling. In the Data Limitations section, the paper acknowledges and mitigates reporting biases in the EM-DAT database by adjusting the analysis timeframe and implementing robust handling of missing values. Additionally, I settled on using annual data instead of monthly, which could offer enhanced granularity because of the scarce number of total disasters. The methodology addresses selection bias by carefully considering the disaster's inclusion criteria and implementing a weighted severity prediction model to account for varying event impacts. The research handles temporal bias by splitting the analysis into pre and post-2010 periods, showing the severity of heat wave events in recent years and how three significant outliers (in terms of total deaths) make it challenging to train the model on the historical data preceding those events. On the other hand, including those data points in the training data would bias the model, even when using the logarithmically transformed values, resulting in overprediction. The paper also addresses potential geographic bias by focusing specifically on UK territory while acknowledging how this choice might limit the generalizability of findings.
#sourcequality
The research relies heavily on high-quality meteorological and disaster data from authoritative sources like the Met Office, the UK Centre for Ecology and Hydrology, and the EM-DAT database, complemented by recent academic publications. The paper demonstrates careful source evaluation by acknowledging data limitations, particularly noting EM-DAT's pre-2000 reporting biases and discussing these implications for the analysis. The academic sources are current and relevant, with key references to 2023-2024 climate modeling and disaster prediction publications, including work from established research organizations like Google's DeepMind on the most recent developments in AI modeling. The methodology section strengthens credibility by explicitly detailing data sources and their limitations, enhancing transparency about potential data quality issues.
#professionalism
The research demonstrates strong professional standards in its academic presentation and methodological rigor. The paper maintains clear scientific writing, notable in the detailed methodology sections, where I present statistical concepts relating to the nature of the research topic. The research follows a structured approach in its systematic data analysis presentation, moving from broad disaster trends to specific statistical findings. The visual presentations of data through plots and figures make identifying key relationships between the variables easy, with clear labeling, appropriate scales, and informative captions. Strategic decisions are clearly explained, like choosing the logarithmic transformation to present the number of deaths. Each section of the paper builds logically on previous findings, maintaining professional cohesion and advancing the analysis from initial data exploration to model development and future recommendations.
#audience
The paper effectively addresses multiple stakeholder perspectives in disaster prediction research. The introduction frames the problem for academic and policy audiences by connecting temperature variability with other weather data research to practical disaster management concerns. When presenting technical analyses, the paper provides clear context and explanations that make complex statistical findings accessible to a reader without knowledge in this area. The research addresses different stakeholders' needs when discussing data limitations and directions for further research and model development. The State of Natural Disasters in the United Kingdom section refers to relevant metrics, such as one-third of UK households reporting storm-related damage, with average repair costs reaching £750 per household, underscoring the relevance of our research and making it applicable to the real-world challenges facing the nation.
#critique
Throughout the paper, I analyze the research that challenges established methodologies and datasets in the field of natural disasters. The paper points out how Mohanty et al. (2024), while identifying novel trends between temperature metrics and the frequency of natural disasters, overlooked critical severity metrics by focusing only on disaster frequency, limiting their findings' robustness and applicability. Moreover, Jones et al. (2022) found severe missing data in the EM-DAT database, also describing how numerous research papers choose to ignore that limitation. I suggest improved disaster reporting systems might create an artificial correlation between temperature variability and disaster numbers, undermining previous conclusions. 
#dataviz
The research uses targeted visualization techniques to discover distinct patterns in disaster data. The Flood and Storm Severity Risk Timeline visualizations track prediction success while highlighting mortality through color gradients, revealing temporal clustering of high-severity events. The Top Features analysis breaks down predictive power, with mean rainfall and maximum temperature emerging as dominant factors. The confusion matrix maps prediction accuracy across severity classes, showing strong performance in moderate cases but weakness with extreme events. The Correlation Comparison plots split data pre/post-2010, using dual trend lines and confidence bands to show weakening relationships between temperature metrics and disaster impacts. The Total Deaths trend analysis adopts logarithmic scaling to maintain the visibility of historical patterns while accommodating recent mortality spikes. For rainfall analysis, the dual-plot approach contrasts event frequency with mortality, exposing how increased rainfall correlates with disaster occurrence but not necessarily with the death toll. Each visualization choice serves a specific analytical purpose - from capturing temporal shifts in the Temperature Variability plots to quantifying predictive success in the Distribution of Severity Classes. Moreover, the evaluation of the performance of the prediction models is aided by comprehensive visualizations.
#composition
The paper takes advantage of carefully applying clear and precise communication throughout the research paper. I break down complex methodological choices in the Data and Approach section into concise, accessible language. For instance, I balance technical depth with clarity by explaining data sources and analytical approaches and carefully selecting structures to explain intricate statistical and meteorological concepts.
The composition is particularly relevant in how the paper navigates between technical detail and narrative flow. I use precise language in the Overview of Existing Solutions section to explain complex AI and meteorological modeling techniques, making sophisticated concepts accessible to a broader audience and relating them to my research topic. The writing maintains a consistent voice that is both academic and engaging, using strategic paragraph structures to guide the reader through joint arguments.
#descriptivestats
The paper effectively applies descriptive statistics throughout the research, particularly in the Initial Data Analysis and Correlation Analysis sections. I extensively use tools like mean, standard deviation, and correlation coefficients to uncover temperature and disaster patterns. Moreover, I moved from initially defining the temperature variability as simply a difference between the maximum and minimum temperatures to referring to it as a standard deviation of those differences, making this metric more robust.
In Plot 1, we can see the breakdown of disaster types, showing that storms (45.5%), floods (34.3%), and extreme temperature events (14.1%) account for 93.9% of incidents, which motivates the decision to narrow the focus down to those three target events. Additionally, the seasonal heatmap in Plot 2 highlights the frequency of disasters by month, emphasizing trends like winter storms and summer heat waves.
I also use Pearson and Spearman correlations between temperature metrics and disaster-related deaths. By applying logarithmic transformations and analyzing different time periods, I aim to identify subtle relationships that basic descriptive statistics might overlook.
#algorithms
The research demonstrates an effective use of predictive modeling for natural disaster analysis, particularly in predicting flood and storm severity. A Random Forest classifier captures non-linear relationships between environmental variables and disaster occurrences. This algorithm aggregates predictions from multiple decision trees to utilize complex interactions between temperature metrics, rainfall, soil moisture, and the target metric of disaster severity.
The model uses a two-stage process: first predicting disaster occurrence through binary classification, then estimating severity levels across five categories from No Deaths to High Mortality.
Key algorithmic decisions include using 200 trees with minimal depth constraints to balance flexibility and prevent overfitting, which is inherent when dealing with limited historical data. The ensemble averaging in Random Forests enhances prediction robustness by mitigating individual tree variations.
#cs110-PythonProgramming
This concept is applied through a sophisticated data analysis and visualization framework that transforms raw climate and disaster data into meaningful insights. The Python implementation demonstrates advanced algorithmic thinking by leveraging libraries like pandas, numpy, and seaborn to process complex environmental datasets, extract statistical relationships, and create interpretive visualizations. The code systematically handles data preprocessing challenges, such as managing missing values, calculating temperature variability, and transforming time series data across different temporal resolutions. I utilize visualization techniques to reveal non-linear relationships between climate variables and disaster occurrences, improving the nuanced understanding of the relevant weather metrics and their impact on natural disasters. The programming approach is a critical analytical tool that translates complex environmental data into actionable predictive models.
#cs130-decisionbrief
The research aims to incorporate analysis that can be presented and utilized at a decision briefing. It translates complex climate disaster analyses into actionable insights accessible to policymakers and experts in the field of natural disaster prediction and prevention. It bridges technical analysis with clear communication, using strategic visuals and a structured narrative to turn relationships between the relevant variables into insights about disaster risks.
The paper progresses from a broad climate change and natural disasters context to specific UK-focused findings.
#cs110-CodeReadability
The Python code is highly readable through a systematic data analysis and visualization approach. It follows clean language conventions with clear, descriptive function names that immediately communicate their purpose, such as creating disaster distribution visualizations or analyzing risk effectiveness. The functions are modular and focused, each handling a specific analytical task with well-structured parameters and return values. Consistent use of type hints, docstrings, and inline comments provides clearness about each function's purpose, making it easy to go back to specific code snippets and quickly understand their context. The code's architecture reflects a strategic design philosophy: break complex analytical processes into smaller, manageable functions that can be composed and reused, making the entire codebase more maintainable and intellectually accessible.
#cs162-deployment
I published the code on GitHub, making it accessible to any researcher interested in this field. My priority is to publish a comprehensive README file describing how to clone the repository and listing all the data sources to make them easily accessible. Additionally, once I create simple interfaces demonstrating the model's capabilities in a more user-friendly way, I will deploy the prototype on Vercel to make it simple to access and interact with. As I iterate on the model development, I will structure the changes into concise commits to track the progress and identify any bugs quickly.







